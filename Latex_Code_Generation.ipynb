{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2U7ZZ9Flch1r",
        "outputId": "fa4b2a66-72df-432d-a518-79dccdc06e0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: xformers==0.0.29.post3 in /usr/local/lib/python3.12/dist-packages (0.0.29.post3)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.23.1)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (3.4.0)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.12/dist-packages (25.1.1)\n",
            "Requirement already satisfied: unsloth_zoo in /usr/local/lib/python3.12/dist-packages (2025.10.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (5.29.5)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.35.3)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (0.1.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.10)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.12/dist-packages (2025.10.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ur6os3YTdA-5",
        "outputId": "5b961e58-6912-46ff-cba4-25c89e3eddad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.8.0+cu126)\n",
            "    Python  3.12.9 (you have 3.12.11)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========\n",
            "Switching to PyTorch attention since your Xformers is broken.\n",
            "========\n",
            "\n",
            "Unsloth: Xformers was not installed correctly.\n",
            "Please install xformers separately first.\n",
            "Then confirm if it's correctly installed by running:\n",
            "python -m xformers.info\n",
            "\n",
            "Longer error message:\n",
            "xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.8.0+cu126)\n",
            "    Python  3.12.9 (you have 3.12.11)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastVisionModel\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "t4h4XG7gd7Rs"
      },
      "outputs": [],
      "source": [
        "# These models are present in unslot module in 4-bit quantised form,\n",
        "# so if model weights are 32bit -> we are able to use 90 % less memory as compared to others.\n",
        "# if you dont use this, you wont be able to run in google collab.\n",
        "# 4-bit quantization is generally a model compression technique which reduces the precision of these model weights from 32 bits to 4 bits.\n",
        "# stardard one needs 16 gb whereas, fourbit model needs 3.5 gb (4 times memory reduction)\n",
        "\n",
        "fourbit_models = [\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj2KxfsFeFM2",
        "outputId": "bb12dc4a-dfc7-4ee1-920b-1f34e425903b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.10.1: Fast Qwen2_Vl patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/Qwen2-VL-7B-Instruct\",\n",
        "    load_in_4bit=True,\n",
        "    use_gradient_checkpointing=\"unsloth\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tpJBIbyeFUF",
        "outputId": "cfa6661c-e24e-4c3b-e67c-554bbda5286e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `model.base_model.model.model.visual` require gradients\n"
          ]
        }
      ],
      "source": [
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers=True,\n",
        "    finetune_language_layers=True,\n",
        "    finetune_attention_modules=True,\n",
        "    finetune_mlp_modules=True,\n",
        "\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    random_state = 3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4hxPYIu4eFhH"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"unsloth/Latex_OCR\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLvixl-4eFoM",
        "outputId": "b85f6bba-6cd6-4e79-9dc1-f679e0534012"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'text'],\n",
              "    num_rows: 68686\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NMclTbIkUhe",
        "outputId": "b5998821-855c-479a-b882-38b1ed982725"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=160x40>,\n",
              " 'text': '{ \\\\frac { N } { M } } \\\\in { \\\\bf Z } , { \\\\frac { M } { P } } \\\\in { \\\\bf Z } , { \\\\frac { P } { Q } } \\\\in { \\\\bf Z }'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#REPLACE // -> / for the latext text to run in the latex compiler\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "id": "x5_dK8GZkUkl",
        "outputId": "288ce78e-9360-4b81-ab3e-14dc5e1a2667"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=160x40>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "#Normal OCR wont let you extract anything from the image.\n",
        "dataset[0][\"image\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcXdgZgCkUqJ",
        "outputId": "17337e98-eece-46ec-c2fd-ab0616490e0a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=320x50>,\n",
              " 'text': 'H ^ { \\\\prime } = \\\\beta N \\\\int d \\\\lambda \\\\biggl \\\\{ \\\\frac { 1 } { 2 \\\\beta ^ { 2 } N ^ { 2 } } \\\\partial _ { \\\\lambda } \\\\zeta ^ { \\\\dagger } \\\\partial _ { \\\\lambda } \\\\zeta + V ( \\\\lambda ) \\\\zeta ^ { \\\\dagger } \\\\zeta \\\\biggr \\\\} \\\\ .'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "dataset[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "9oPemxjrm38L",
        "outputId": "7e889f57-fe9a-4a59-8047-e1c5af24c87c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=320x50>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAAyCAIAAACib5WDAAAYrUlEQVR4Ae3cebSuUx0HcCSFEBHKlNZCQi2zureMyTwrY2UeQlSaXNdQxjSSMltFSJF5bjCzDMuilUUUIjSKotLtc+83+z73eYd7znve877n3vXuP56zn/389m/av2FP75l10qRJswzKQAMDDcyYGphtxmS7P1wPgl1/9N4nqoZ77I/4TOvA3dU+bP/5z39mnXXWPtnSgGwfNGC4lf/+9799oD1kkjOnA1N6tD9kPUwHELbZZ5/9hRde+Oc//zkd0Ol9FgteffXVsR/apyfHTP6dCT3zzDP/+Mc/ZpttTPvImGauYxuh9BdffPHvf//7v/71r46RpCNPM5Zc7uijj37729/+ne98J9m4Y7Riwete9zpPKb1jJIOOo6qBjPinP/3pt7zlLddcc82///1vBjCqFDtHzhxnpsIraP/nP//5IossMvfcc3/5y18mHTfuWMZky0022YSKf/rTnwrJSHSMTd8//vGP55133qabbvr888/Dg9uOsQ06jpIG4sCvvPLKxhtvbNzFboS48SiRGwnamTADS25i5+abbz7PPPMYg85j2yyz8F7J/IYbbrjyyiu/8pWvwPmGN7xB/uwAp0HSy7xggw02uO2222688cbXv/71HeAZdOmBBpiQYoAuv/zy973vfccee+xf//pXa6gMYg8YGAaJkXj/WOub3HjVVVfxsb/85S+m0JasI2EyqVs4gNACmD8rI0Go+9/+9jcYllpqKalYZZCBR6LPUe2blHv77bdzp2OOOQatkUzlRonV2Yfh62MelDNImBxYsJxzzjlly66wbCrO8cRjyA3DSHCK6/POO6/JM+NQHwmqbvUl0RjhpFsSTRdPBnG6UovaIFdffXWQpk7TRdsXgJlnCk3XfEyMPPPMMw8++OA55pgj8XLkahUXIMmojxwbPCxj5Hi6hWG6dtwtQmMHD5GHKDWwTOLG1JBVNdmJAzNoCaSKZezUX375ZRqXMKeM0VDHqZf8D9F0esOSaCK3ZBrfG4pjgQojUYYYkU278DxE4N5L14kDE6mLGzBU0xXtWABT33e/+13PDTfc0DOq771O21MkbFJ6F91migqHp8Z0sTez8sorW94LK1qqnAeg2tL3urgc1TXlxCcATT+Vxixrv/SlL33xi18kcmymfJ0RK00cmFTkVFQoRUmLp0kpIR3STJw4EYB6bdSHpQJ94ZycJae971IolvEoLWGpFRUILS/NdtZbbz0w7R0YqqqNtsfciuJw2yPs/PPPb33umdfhIqnBFx3W1FgDa3wFjw1jqlLdrqdtwIW3kQxxI9HGFvxPMbfJ9la+ptEzzGhnDEa2OqbVrwB8AlBspqAqFYKAIReYPAvyAjPDVZo4sB0gCVZRIbCSFk8LSzJffPHFRx11VFXdnYlNiXBKAgoqBUmhWBYepSUsFchSMTYYlknOPvtswwNh+dSqAlWxUTCtMLfq3lk7S3388cfPPffcxx57zFM9cbAzbHoRHOe23J9++umXXnqpqsamOA2f2SN31fGCCy5Ya621jOl73vOeww47jDYwEyvXF8I//elPKto1NsWWRthEgTb2oHsbDPifYm6T7a1QSaNnJIr3/vjHP77++uvBBFv5Gj/0CUBTHw6HCRNO4J3nX3311Ysttli2poItmgHTJgQU9sZOZarKwpOROO200/785z+TiiRbb7219lxGIZvbEXvsscc555zzqU99yjavFqrvQBjIFR0///nP/+hHP1I/5ZRTHJozAght3N9///2sxwbghz70IWBarrvuOpaEpQ9/+MNrrrmmLo3G6uDXIY30izfjAb4Vb4z41FNP/dnPfrbffvtB+PGPf3y++ebbZpttxo8fr2MJHK26d9wOuSNl04TDDz/8d7/7nfqee+7ZMTYdyXjkkUf+5Cc/eec73/mLX/zC0Gy22Wa01CjCZI1PmvTkk0/+8pe/3GWXXfR985vfTN7vf//7H/vYx3JjIZzcddddAjTH/tWvfrXoooti8o1vfGNoBaA8jRcvEol23nnnueaaq7TXKm0Ggr1973vf+/3vf6+7Kclee+1lUsDfTj75ZPEI/o022shwE8ftl69//euXXnopKfQycG9605vKkGl817veteWWW7LJj370o0UDNQ7dw1lppZWeeuopA7HPPvuwE6xO1su0mqnxP6Zfw32epCI55wnHZ511ltDOJahVi+QmYwhd6txJF9qpdh96HRXAa6+9ttj/3HPPPfLIIwxF1MCAAeC6biyiwm4SSjzTgiVf0QVZJRdOEp4du/tkjKsApa6jXS5fP/GJTyDhvqtPv/nNb9T333//Wscg4WyChbH3tUa3oO19JSJLJjjHP1ZlVG7geBmTjXymhaplXZJedtll4XnxxRdPxaAo9L/ccsvtuOOOvOjZZ59997vf7RgcQKM+IQTv0yGHHLLDDjt89rOfbaUiCldCpfZE7uGHH+ZUpHjwwQchRBfMgQceqEVoMNxePR2/uf2irgswU5jqkFUNoGigkUNXA2DA6qGHHhpUnjXNuLyhsdgYoQQga2aNjUrQ2N8ydeJKHQpL5VTS4Gc+8xkmLgyLcwsvvLBP0pTrB4L9Cius4DoRJVbDvFeabVpIOAX3/x8gUfnhD38oOggWCy20EO8VbumRpnxdYIEFjAE7A3bSSSdpEZv5ueSPJXV0a0EdDOx33HGHp15VctV6SIgLBGSmKi67alx66aWdPIlQ5t4+1RhmUhnjKqqO6yxDoahUOsaDJX3vvPNOHoh/bH/kIx/BqrmMdvirmOmHxi655JI11ljD1SIC0hIM4pcn3QabRnMTWdr4Gvq3vvWtPPMb3/gG95Npq2pRhxM5OfzXv/61QSRObVAKG+bnSnmtMgbPMsssI4HDb0QwYHDNbO+77z5Ts1133dXMCLzuku26667LhUAi/Y53vKM6ZDpqXH/99QlIAzihgUYOkdPOjRWVSNGomSqH6sB4QVRU+9T/V8yVYgzUr732WiKZjJV2NxmoJucNJld8mGoCXGCGXkkYk9ZM3qTWP/zhD8zFPFY7nDBTrtF69NFHGZBZMeuha3nAJMrX6L1Kjma1szzzbXacpK2xClPqIoU1gtD+gQ98wDQMGPyeN910k8FgrCDDYWiZgIj9jJ6xYq+KVr1fBZNIe2KSoR900EHGxcycHbP18lVFAaPQKiu86KKL0pinzOYXGslykUW7GabwLUdZYpxwwgnU0qjzqAiMcN/UEoItsc9kVdGS1yoD8Gi3LYxKYUCG5IfA8pXhielm8qQIrQxEbcgCTA+rrrqqugJDjcMI8slPfjKzLTbTSjP64seTwZhpM9Hw4zmmyjRrYMKIf7feeisWb7nllrvvvptapSkyyIfcmPzmEtXEG717mr+54suAiO21FIGcEi2e4RQXPBNiWQls1sDg5QHZmO9hAAZgeLCoM49ijg888MAqq6xy/vnnW+zla0FeKtphM4VOdi3tpRLRzBsF+wMOOAB+S0H7GWhFHPN5TFpiGVrYwq1gbwpgb+zEE08kCCTgC85qvTT2poI9xexup512MuWjRiZuZWsBabBkVMo0VSnMkMhFbqJtt912SadayjhGENKR1xIJTsq3bhJhH3roIWAGSBQzP4cWTpAq3Nuc5YknngDAWyDUvVAMTkrTkqeWVApMKto5pDqHNAomQcKrqT4qcPpK/1xI+vVKapAoqtSGDKRimiYKQ2JSAGGNw3RncpEdQqWmGZ80ogIbo0L6c5/73BFHHCGC+E0LrtI3zPf9OVXjZDMAYpKJqFBN11rIwCtwPG7cOLwSTJ38kTDcgzF+VjKkVY+O8gkYczEYxYGDRCSWJ0V3eVVfc/JvfvObjqYMJPwyfHZZ9t133+OOO+4LX/jChRdeKCTrizQSQV57winEEKHKQGC04MSg2uSQMfwswTrZls/uu+8OwCcABuZtb3ubEzJgCy64oBZ2Y+FtN4Wfy8Bl1AtOkMBqIte46u5raHkaIC701a9+1RqYYWULyg6F7UZGRmNOCriW+SeVxi35m0ZSiHHBE+YJbuDwSfMkIqyT4cRuK2GE7Ir99re/pTrxNMeHNAatOAgYNiNY80yYo5xQF2jgN+J55RJKNAOVinP7448/3tTPBuS2227LA8ULjEXnrtbZEtc3XTy1e9aGLI34YQZkwUMjh5GUQoQ5GOhQwKppplBRiTmZj5jS2xtjhzbV0IW/CtbPOjlTyK8SLeD4teZJzB1/5Wtp76xiGHQ0JJnpBcl73/te29rqJkue9G5OmE/qlMWksovAS9NefTIgr1ydXYqUWE1Lgcmr++gE4beClAl8xgYMlnQxR0qMkGyNkPavfe1r4KMKXaIB7amYkVqzmX86jfAcYhEjUoYIX8CqvZZddlkugSVukKmdVzzzWxaJQ5+owrxUI0vVYrFgL5ekitcUX8kiZsW1NEZF9vzV2ToZaSD3+IVazgYy4ltZyPb2Mr1WceZrTTk8SiHLkksu6WlSE/yegVdBSLt77CuuuKLX4MzAkctXMgYe2+G8NmSxDbsndCX4gm/kMORMKEwwQ6KpZhAqpXCYfbUsuEpjwPCDVc/Sq2eVqRkYT9R08803cxgHDMYvhQOvs846+E7kBlMtGGUrLMYsDkz1kzpUpBLChWoVr6h4mj8vv/zyZGZwBttWlrgL3iskNj8TNbTo++1vf/vee+9Ni741EuU1DokKfkpjtWJEfeKlxlWwNx7yjB0UacoK3EQDP5b6ZtGcVsfddtuNXOjapDFNxXkw58kiLRmIDzJKqNJKXTt+1PGWFqR1Zzp0GzwFUqXWUn2tdTQW7BV7VvLYphbSiTgmeygCFu+8WhCChJlicxgD2GvUmOz3wQ9+UAt+ggRR4RKYpCqW6bj33nvDKeB+61vfOuOMM6IcIth6hBM8zUAVbvMsygFGdlwhIcfmNem36MQngRt152omYqeffnpMUXuKqTu00SSVouUpE9aGLEJ5hge0zPtqHOaTdiUiN9XMa5Qnj6xCt5YV9sBNdug2NlxgVLCkVFt6Vw+LnuTxFJDQLo3xyYTMptmPMICBidCWWLXCSbQQGwyFAjYMjMPqhQuFSkbXjCuoRFAbV/kUljJ711FjYPI1z+DEm2mYwTCo2tGqwqDoFYcRLSdhVtRSqB3XnHVfccUVYEgKxpxKPRRzTiNva6mh1TLcIvbbtAs/w+rb2DGTQPOO4IkaZZ6wLSzyIoJEY056yJXNqhrdjEtNRWC06CKNq8PmmWSlAieX81WYYNxa2hfTKKUpTNgz+riFsPYPGPLV/FlSZWMw4FZFPgDcOGQAHClzTnoI/005hDZ2CL6NZnyFBLBpXSFXs4G8msUIcJ7h0LNnZaqv8gEx2wzHdohAaMxsx2+//fb2QqRB2YbMykg4y3gYe0tKOPm2nQmBPCdv3BUV7oQBnOAntsXfDJhX1IOhykNa7IFRsdAen6+BFTzCpIMo2xumGPZpDJ4NNh1zrA1/8Ni/QUJ+CzbrMXt4hZ9CPV+9CjppDIeoR1EcxlpA0VcjkWF2NGJhKdBoiX0Ac6qZRWMwaLGCZYWezHerrbZq7Ii6+YJzWr7hq6M+sSZswJzb4KZOaREuiUxSLYTldeEwXz2LimQw+0BwCrLmqMJEbNT+XzKnWAY/DMSR0GT4iRMnYlhjwabiVYngk933gAPKaw0Mfqq2N2HyzOqik8DoosJgnEeYXefVXK/VkAGQ5+V27CHdnsPI1UYzIUcDZamCtyrz6mmRDLDkWVpqYKP3OtWBjahdEBZseaPCPjzFM2Yk/Fv8YCIiNeWGJK1KsZWozGUMokLoaMGpBvwFoUYUw0DpVb42pZ5GmHkFG9W3KZ/BZrQcIaDC5YRVkKw5dk/ewDgstRXkE5wk0p7kZiCrmANsciHiWNqZiEKlUYmYtuJMzgUpIcMJLcaMsW0kSCwfZDMVyQT/whaFOJFmcCGhxam7mYgAauamY45/SkcWDxIhgR8JkcgZkhbc8gHLBI4qLOpos127wsd4mumoRstadEmXT3niXIUUOWUJq0jAKdfpZXExfvx4yIFFRrGVk/skfCRL18YoJNz0UvSqUdSiBBUxlfI65cvkR7rYTlPyFTO5hVIbspAWak211Dkw+OlyCKaVZoJQLIjG6Kcp/zDgBJineo/LVAfuAeHIb0/CJlaVXEyn2lKt02NUWW0s9TL8soHYDLKVlttQafUpqOx4Sd1GCNFwgqgKV3//+99vxc558rNv4U+7osWc0MF1+JTzJVIhI6sJXpTwYchDwjqflSiZvevldBfyfJ0wYUKtY1MZccWLINExdB2/ueSESqxZI2O1xKUo9UYkNT0kf5qFwpn56k1TTsvjabFXocQNZLEmgZjsIZ1nRgfz4T+vVYBavdbd13Thh5JwkUJ7jdXgMa/BqlHwOnkYpjDTnsOCp1Ez6W7cmRYlwNmosdDt43MaB54i8v8fRQV57wqLlCXnuP0bQ6cOwxM1FfyFh9LSvhKdZlrlWAVwGy0j52tIxP60KFUS1ZagkuWYRdWBY0kylXZTO93t96iXFakkoFfQOiHnzElQWri616RQnGBDwT8f1m7TBSHtkMfNQrexoxZgOEyJIVKvE7JI5GlV5sYbsLiiPG9qPW7cONEk3T1rRa8gLHqQyeVeYFqgsrrRXUUxkREsTByEGAwQpIZt6K+FXGMX/Gi02LH9Llx6rRHySkDbGQIWeQGHVZXpcqgvQZpqRjsM9G9kM5rhRGNjCXBj+2i3TOPAo00MfsNsDGJtXSEXnfIcWi6htyuYIQnyRgdmH77KKg5gpSavll7FgUnHyhVrBFu49i3j5NpNOE2n9XXY7hk8Zte2HtAy740U/NC61w1nZqGX4pJZtaMW3duUpvYkavA9qaZNx1afwmojWh5i1dPGsiHUN91bIW/fHiVwJDe0nJIAromfV5P/Mu2vIhwKh001E2GH6MBVir2s99qBuysbFbMM2rcZZg2cZNVoZB0TbeXAjQjtJ/G97BSwmNVWW+0HP/iB1antJROwmJ0rfmBYoQWhe7yQJJPbwdpiiy28OpwEkHNXW6lZz3Nmy85axzYOk5QSDqc4zjTzi7QP61lDUsU/LDy9AR5JpKhxOEM48Owso8eFmth0t4jaNWVS1mYcw46ousOkbiFvj4cgyDkvtafiX89Ks7m96CajzOzkUIHB7w1sQY0bN841I7f2uLcVI4/1CQZPe/5LLLGEug0Y57pSNzxuiQlJcgsAedv9qmrHNqeO1fPVHI3CUEooDkv/NSRV/NB2gLAwM6wKQpwTM61kj7/VuB06hz0TZFhSTxe4Dw48LOuZrgAAZGBWZcu365jbU2dPvNcUmjfa3zbLlYGdgbl/ixO+J8GKJtZmiSmZ51dxsjZIfHWCoovixqJ9Y3dI/TNxlipAiErq1V7qjWZaA2j1ikSrT521dx1hKzYQah+aWzn2EDkcIlgr9vrV3qfrI90Ql3HzEJj8NJ8b5ApKLT90g05zHPyTPdn2lFT90IL3urWS3+5Y9DoE4mN2m++55x5enVNZgQbPKUkXuAVmi9VPLJCRBFwCF4l05MNaIs5rnSb/1bE5Q4PW0dFAFD523ZvRzOjFhq01Jx9WuitLlpqOkdiGtSjkcTyEVJz0xmbKVX6nRz65+MH9+K09FT9b5b06ZqOlsKc7SDD+HwUk7soDCLnsqPN2MErpMqj0WAPRv3HhvWP2GKkPU+gRBkqjSKFykc0eZxiuFvABN5alMo0dzy3bcOUKR5DHUROMPWVaVyMkSb5t/mau6zKGnzG7deTQ0qzYRWsZdcKECUwhvQoVUujilrVr4QKEnze6pBH8bmXI5+qBKV0GlR5rgOsaXFsYBsJ+RI+pD5Uc5maskrjo2gAJXZDI/5SSytLeXVmS0nM9QIyAvJZIG8nJn25uaXda1vh10DKjaCDmZLhd+7Wi6foJZbf0MOOtgaUyyhUR7RtJuTJYrrPSSC3LDTWGtYaTeNFyRGQT2LV+u81W3QY1PXzCQCkmvT4BkHV5viQMzNfW6Cf/SBMkGM8CRpBCojQOKr3UQLzLuLij4lqru4Nu6RqjrhtYF4TqViToC56clCLNE0aJAZi5k8Hz0zxRw2Wv+G1TcubS/gFAdbXcFGzQOMY1YMTtR/h/YJZFthjVGcDY5HlWbHUhDPQDBS1nxVsqo82Febud5/woZ7RpDfD3VwPCtEVQTgT6y0l76jOwAxNM9OnNrAYhYSKHOu0VKlQPBaw9ksHXsaMBntzqhHksMDljO3CPNdizeNFjuQbkmmogk9PeZIimDAylceDAQ9HSAGaggTGqgRlvF3qMKnLA1kAD/dDAwIH7ofUBzYEGuqSBgQN3SZEDNAMN9EMDAwfuh9YHNAca6JIGBg7cJUUO0Aw00A8NDBy4H1of0BxooEsaGDhwlxQ5QDPQQD80MHDgfmh9QHOggS5pYODAXVLkAM1AA/3QwP8AGMg7qICuIqsAAAAASUVORK5CYII=\n",
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAyAUADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iq1/f22mWE17eSiK2gUvJIQSFUdScdhWYPF+hGGxmW/Vkv8/ZCI3PnY5O35eeOfpzQBuUVk6DrsevJfywQukNrey2iyMQRN5ZAZl9t24f8AATWtQAUUVkxeJdJm1dtLiu99yHaI7Y2KeYo3GPzMbd4GSVzkAHjigDWorOTXdMk1afSlvIzqEEXnPb87wmcbgMcjPcU7S9ZsNagefT7gTxI5jZgrABgcEcgcg8H0oAm1HULbStNudQvJBHbW0TSyueyqMmquhawdb0/7WdOv7A+YyGG+iEcnHfAJ4PY5rnPibIZtF0zRQpb+2NVtbNwoBPl797nBPI2oc9euO9dqOlAC0Vn3Gt6fbal/Z0s5+2fZ2uhCsbMxiU4LcA98DHXJqbT9RtNW06G/sJ0uLWdd0cqdGHrQBl2HiJ9U8SX+nWVkXs9Obybq9eTaPOKhvLRcHdgEbiSMZHWt6uO8Cf8AIQ8Y/wDYfl/9Ew12NABWZea5bWWsWOmSRXJmvHKRyLCfKB2O+C/TOI24GT045rTrmfEssq634eaOyvJ0tr1p5nggLqiGCVMkj/adeBz3oA6aijtRQAUVU1LU7PSLI3d7N5cQZUGFLFmYgKqqASxJIAAGTTNL1ey1iGWSzkZvJlMMqSRtG8bjBKsrAEHBB5HIIPQ0AXqKr3t9a6db/aLydIIdyoXkOFBY4GT25IrCsvFE9341vNEGm3YtYbaKVLrygEJYyZbdu5VtoC4HUNntQB0tFYq+K9GbUxp4uyZmna2DeS/lGYDJj8zGzfgdM57da2qACiq1rqFneyTx21zFLJA5SVFYFkYEjBHUdDVnNABRVe+vbfTbKa8u5PLt4V3SPgnaO5OO1UJfE2jwaRbarJeotjdMqwTbWxIW+7jjJz29aANeiqP9rWraqmmozvctB9oZVU/u484Bb0ycgDqcH0NQw+INPutKudRs5HuYLZnWURRsXVk+8u0gHcPTrQBqUVVTUbOXS11KO4R7JofPWZTlTHjduHtjmud1Lxi1v4k0PT7GymvrTUN+65t0EiDHAw24D5Tkt146c0AdZWBa+JH/AOEsn8PahYm1nMTXFlMsm+O6iBAbHAKupPK88cgkVrTajZ297DZzXMcdxOCYo3YAvggHGevUVy2u/wDJUfCP/XpqP8oaAOyooooAKKKKACiiigDi/ilfrb+CrjTkuEiutWePT4dzAZ81wrnkjgKWJ7euKxNMtp3+Kmm6bc61Hfw6Jpsk0SpEkflyOREFwpOSEVuvIB/2q9NZFb7yg/UUBEByFUH1AoAr2Gn2ul2KWdjAsNvHnbGvQZJJ/Mkn8a8/fVvExkYi48QKCeAPD8fH/j9elUmB6CgDIj1KS08JtqV55xeC1aaXzofKc7VJOVGdp46Vw2gxx/8ACV+Hre11Y6nb2tjPeXUStGYLKVgoDgoB8zF5eHLHDMeOtejajp8Wp2TWk5YROylgpxkBg2PocYPsamit4IFdYYY4w7F2CKBuY9ScdSaAPJ9cldIJfiNpEaX11p2qzKY4Hz5toALdk4908wdcZJHBr0vw/pzaT4fsLF8GSGFRIR/E+MsfxYk/jWiqqowoA+gpelAHAa+x1P4xeFdOUhk060udRmTGR8w8pCR2wc4NdF4z1m48PeDdW1a0jWS4tbZ5I1YZG7HBI9BnJ+lYugw/2h8UvFGrNl1sobfTIXyCAdvmyAHHq6ZGeuc9q7ZlV1KsAykYIIyCKAPIdD8T6ZpnjDW7/VNfl1aWysobSGSNQ5lkJDSrGFGPmkaMKo7hh0XI1fBV3rGlweIdCOnww6mhOqafYTTEIsVxlhGWx/BJvUkcZ/OvQ0sbSMIEtoVCBQuIwNoXO0DjjGTj0zUhij8wy7F8zbt3Y5x1xn0oA5H4bLAfDdxP50kuoz3076n5qhXW63YdSoJACgKBg/dAPetS4u/FK3Mi2+kaTJCGPltJqcisy54JUQHB9sn61leBP+P/AMZf9h+X/wBEw1z3ivSlTxFGttqivq7alDqDXcoCHTLQYVlZ+6MRtVD94k8cE0AdbNqvii28vz9K0KLzHEab9YkXcx6AZg5PtVaLxJrdxftYw2vhuS8XdugXW3LjacN8vkZ4PB9K5jxvDc3Y1bVvKtZLXz4tJV5kZp4kZ0RzbgjaHLO3zc8ovPGBt6ZoWpJ47a+n0mK3062e5+yGK5UjMpBklZcbjI7AcZCqM8EnNAGwL7xac40bReOv/E2k4/8AIFU73xLremKrX9r4btVYEgz626AgdTzB0GR+dcnrtwNA8SS+PyJRp0d8+l38aJkSW21U345BKzhvc5x2q3eeEb1PCOlWml6LB9quYJRqNwkqQzRLMA0yJuXGWPy5I+ULwM4wAX/FWoXV3feGNK1Oay0yG8kmurm4imWQJ5O0xrHJIgAZiwO7AIwcetY+jardeHkuvFUt2kuk6vriW5e8IWV7UKIIplORk7lycglkG71r0mHTLabSbW0vLG3ZIo0HkOokVCFxgbuuOmaq6n4V0nWbqW41C3Nw0lo1oFkclI0bO4ovRWIOCw5wAKAMGa403VfiTdabrUtoxsbaI6fZXBH7xpA3mShW4cgKFGM7RnpurdmsTpepaprkYWRTp8USW6rg5hMrcH38wDpxiodX8HabrVjp1leNK8Ni8bIWCu7bMYy7KWB+XkqQTk810NAHi+mwS69b+E0t9X+0ahqN6mt6jBbqn2e2Vf3p+RR8jbzGvJyxLZPp0d14zurv4eardPeWtrqNlff2beXNsS0cOZljaZeSQBG+8Z6Hr0rtLnQ7GfT7qzji+yx3Q/etaHyXb1O5cHPv71Fp/hrStKup57C0S38+3jt3ij4jKR5C/L0yA2M+gAoAXQbLRbTTIm0GKzFo8ahJbYqwkUdCXGd31JJ5Ncnq914oPi/w8X0jSllH2ny1XUpCrfuxnJ8njj2P4V1Hhzw1ZeGLS4t7IsRcTm4lJREBcgKcKiqo4UdAO571oy2VtPd291LCjT2+7ynI5TcMNj6igCGSGe/0WSC+RLeaeFkkEEhkCEgj5WIGevoK4H4ZC58Q+G9AvL2J1s9Is1gtVYFRLOq7GkI7hFGxT6lz/dNemU2ONIkCRoqKOgUYAoA47RNQg06+8a6nq8kVstvqIEkrN92BbeIpnk/3icepNZXg7U7mLxzqIurGTT7PxHH/AGlYW8xIcNHiOTcP4XZfLk29geea7L+woRr0+ppJhbqFYrq3ZAyTFD+7fnowBI9wRnoKuX1o13bOkUvkTlGWO4VAzRZGCVz3oA8xtLzTVh8PaPqF7bQ6FLf6kwWZ8RzmG5KwQ7jwV+bdg9fLA56V302gQnVdHu7XyYILAzfuY4wAwkXHGOBzz+NJP4V02bwqnh1IxHYpEsSgxpKcD1DqwJPOSRnknrzWjpunwaVpdpp1tu8i1hSCPccnaoAGT34FAHI+BZdL1wXer3Elpc6691J9oU4aS0COyRxhT8yAKB6ZJLc5pPHbNBrvhq50wtJ4gWeWOztSP3c8TKPOEhyNqgBTuGSCBgHOK3j4XsG8WJ4jYE3qQtCmERQAcZywUM3T+IkDJxWRrv8AyVLwj/156h/KGgDr5ZkggeaVgkaKWZj0AAyTUOn6haarYQ31jOk9rOu+OVOjD1FWaOlABRRRQAUUUUAFFFFABRRRQAUUUUAFZ+uWF3qej3FpY6lLp104BiuolDGNgQeh4IOMEdwT0rQooAyfD2hroOnyQG5e6uJ55Lm5uHUKZZXbJOBwAOAB2AArWoooAKKKKAOb0nQr/RfE2qzwSwS6Tqk32t0clZYJ9qq2OCGVgoPJG3Henz+BvDNxrY1qXRrWTUhKs4uGBLb1xg9e2B+VdDRQBzWl+C9MtGt7u7hFzqEchuGkZ28vz2JLSCPO0Nkn5tueBXSModCpzgjBwcUtFAGXD4c0iHSJtJWxiawmLNJbyZdGLHLZDE9Tz9ea1KKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArmYdB1C78bHXtUlgFvZwyW2nW0JLFVcqXldiB8x2gbRwAOpNdNRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAH/2Q==\n"
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dataset[2][\"image\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "e1AOxSh2m4FH",
        "outputId": "54c3b5d7-b0af-4a8c-a269-22292b8355e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'H ^ { \\\\prime } = \\\\beta N \\\\int d \\\\lambda \\\\biggl \\\\{ \\\\frac { 1 } { 2 \\\\beta ^ { 2 } N ^ { 2 } } \\\\partial _ { \\\\lambda } \\\\zeta ^ { \\\\dagger } \\\\partial _ { \\\\lambda } \\\\zeta + V ( \\\\lambda ) \\\\zeta ^ { \\\\dagger } \\\\zeta \\\\biggr \\\\} \\\\ .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Direct Copy and Compile\n",
        "dataset[2][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VBgl9Xmym4It"
      },
      "outputs": [],
      "source": [
        "instruction = \"Write the LaTex representation for this image.\"\n",
        "\n",
        "def convert_to_conversation(sample):\n",
        "  conversation = [\n",
        "      {\"role\": \"user\",\n",
        "       \"content\": [\n",
        "           {\"type\": \"text\", \"text\": instruction},\n",
        "           {\"type\": \"image\", \"image\": sample[\"image\"]}\n",
        "       ]\n",
        "       },\n",
        "      {\"role\": \"assistant\",\n",
        "       \"content\": [\n",
        "           {\"type\": \"text\", \"text\": sample[\"text\"]}\n",
        "       ]\n",
        "       }\n",
        "  ]\n",
        "  return {\"messages\": conversation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veTeG_Icm4Mn",
        "outputId": "3042f37a-f942-4185-9519-d9695b64aa59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [{'role': 'user',\n",
              "   'content': [{'type': 'text',\n",
              "     'text': 'Write the LaTex representation for this image.'},\n",
              "    {'type': 'image',\n",
              "     'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=160x40>}]},\n",
              "  {'role': 'assistant',\n",
              "   'content': [{'type': 'text',\n",
              "     'text': '{ \\\\frac { N } { M } } \\\\in { \\\\bf Z } , { \\\\frac { M } { P } } \\\\in { \\\\bf Z } , { \\\\frac { P } { Q } } \\\\in { \\\\bf Z }'}]}]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "convert_to_conversation(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eurr59cTm4Pc",
        "outputId": "c8731174-ab79-44b7-d18f-05b6f5c09c2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [{'role': 'user',\n",
              "   'content': [{'type': 'text',\n",
              "     'text': 'Write the LaTex representation for this image.'},\n",
              "    {'type': 'image',\n",
              "     'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=120x50>}]},\n",
              "  {'role': 'assistant',\n",
              "   'content': [{'type': 'text',\n",
              "     'text': 'D _ { \\\\mu } ^ { \\\\alpha \\\\beta } \\\\bar { A } _ { \\\\mu } ^ { \\\\alpha \\\\beta } = 0 ,'}]}]}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "converted_dataset = [convert_to_conversation(sample) for sample in dataset]\n",
        "converted_dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Sn--PdIrm4Tq",
        "outputId": "9209169a-58b3-470c-82e8-ec4494b5465f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Qwen2VLForConditionalGeneration(\n",
              "      (model): Qwen2VLModel(\n",
              "        (visual): Qwen2VisionTransformerPretrainedModel(\n",
              "          (patch_embed): PatchEmbed(\n",
              "            (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
              "          )\n",
              "          (rotary_pos_emb): VisionRotaryEmbedding()\n",
              "          (blocks): ModuleList(\n",
              "            (0-18): 19 x Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (19): Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (20-21): 2 x Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (22): Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (23-28): 6 x Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (29): Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (30-31): 2 x Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (merger): PatchMerger(\n",
              "            (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "              (1): GELU(approximate='none')\n",
              "              (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (language_model): Qwen2VLTextModel(\n",
              "          (embed_tokens): Embedding(152064, 3584, padding_idx=151654)\n",
              "          (layers): ModuleList(\n",
              "            (0-27): 28 x Qwen2VLDecoderLayer(\n",
              "              (self_attn): Qwen2VLAttention(\n",
              "                (q_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (k_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (v_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (o_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (rotary_emb): Qwen2VLRotaryEmbedding()\n",
              "              )\n",
              "              (mlp): Qwen2MLP(\n",
              "                (gate_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=18944, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (up_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=18944, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (down_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=18944, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act_fn): SiLU()\n",
              "              )\n",
              "              (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "              (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "            )\n",
              "          )\n",
              "          (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "          (rotary_emb): Qwen2VLRotaryEmbedding()\n",
              "        )\n",
              "      )\n",
              "      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "FastVisionModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3xMiugeboc3e"
      },
      "outputs": [],
      "source": [
        "image = dataset[3][\"image\"]\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": instruction},\n",
        "            {\"type\": \"image\", \"image\": image}\n",
        "        ]\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5IrUx7McodA0"
      },
      "outputs": [],
      "source": [
        "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = tokenizer(\n",
        "    image, input_text,\n",
        "    add_special_tokens = False,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "YjUDLFaLSQOM",
        "outputId": "597461a4-8cd8-41ef-fa85-31f7c89e03c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=120x50>"
            ],
            "image/png": "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\n",
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAyAHgDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iiqOo6ta6Wtsblmzc3EdtEqLuLO5wB9OpJ7AGgC3M0iwu0KK8oUlEZtoY9gTg4+uDXMW/je2ufCtvrCWU5uLi4+xpYqQZBc7ynlk9Bggkk8ADNWbnxFcXvhkal4csW1Ca4kMVssn7tM7yvmOTz5YwWyOSMY61g+G/Cl3p+o6xpGsxf2lp18YtSN06qqG7PEwCjBX5lDgDoD160AdR4a11PEeiR6ikDQbnkQxMwYqVcrnI7HGR7EVr1T03SrDR7T7LptnDaW+9n8uFAq5JyeBUNzrun2muWOjSzYvr2OSSGMDOVQDcT6dePXn0NAGlRWBovjHSdf1GWysGmdkh+0LI0RWOWPeyB0buCytg98ZHFb9ABRRRQAUUUUAFFFFABRRRQBx/jPVHtNQ0uyn1N9K0udJpbq7iyJWKbNkSNg4ZixOANxCYHWszSfB+o61oWgza5qmrLNDKb2WOebE6ybWWMBlxswrHIHc16GQD1ooA5y30G58OaLbab4WS0SKNjkX8kj8dsEc+30FVzceL1uFt2n8NidlLLEXm3EDuB1xXV15P4tsdLHiNLGDVV/tN9Vg1O5up2Tdp8YwAiNjcWfACx89SeB1AO7g1DUtLsLu98SSWEcEShlazWViB3yCCSemAK86dNSj8c+GfEmqRWZW/vGkCRBzPbwvbskaOSMBF3c4x8zH1r2LtSNnacde3NAHI6HpnhfTdbik0+8le78qSyiie5eRVSFvmRVPAEe7b7Zx1NdHpWqWmtabFf2LSNby52GSJ4m4JByrgEcg9RXJeE/Ad1otzp+o32sXEt1BBKrW6BREDM/myLnG5hvJOSecJ6c9zQAUUUUAFFFFABRRRQAUUUUAFFFFABWS/hfw/JqY1N9D01r8SCUXTWqGUOOjbsZzwOfataigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAP//Z\n"
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "#image\n",
        "dataset[3][\"image\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "E_Q6kzYZp38U",
        "outputId": "59602e86-a9a5-4d72-ffb5-d2aacda2a085"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\\\sigma ^ { \\\\mu } \\\\frac { \\\\lambda ^ { a } } { 2 } A _ { \\\\mu } ^ { a } .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "#actual output\n",
        "dataset[3][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_D_mjFaodEm",
        "outputId": "5fd73519-9552-4693-d113-760ff97b850b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The LaTeX representation for the image is:\n",
            "\n",
            "\\[\n",
            "\\sigma^{\\mu} \\lambda^{\\alpha} \\frac{\\lambda^{\\alpha}}{2 \\mu^{\\mu}}\n",
            "\\]<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "_ = model.generate(**inputs, streamer= text_streamer, max_new_tokens = 128, use_cache=True, temperature=1.5, min_p=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3uvrItuoodJx"
      },
      "outputs": [],
      "source": [
        "# Fine Tuning using unslot\n",
        "from unsloth import is_bf16_supported\n",
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from trl import SFTTrainer, SFTConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "n43cdzP-odMo",
        "outputId": "fd15dc87-8e8f-4dce-bc23-b9fd95505167"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Qwen2VLForConditionalGeneration(\n",
              "      (model): Qwen2VLModel(\n",
              "        (visual): Qwen2VisionTransformerPretrainedModel(\n",
              "          (patch_embed): PatchEmbed(\n",
              "            (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
              "          )\n",
              "          (rotary_pos_emb): VisionRotaryEmbedding()\n",
              "          (blocks): ModuleList(\n",
              "            (0-18): 19 x Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (19): Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (20-21): 2 x Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (22): Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (23-28): 6 x Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (29): Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (30-31): 2 x Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (merger): PatchMerger(\n",
              "            (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "              (1): GELU(approximate='none')\n",
              "              (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (language_model): Qwen2VLTextModel(\n",
              "          (embed_tokens): Embedding(152064, 3584, padding_idx=151654)\n",
              "          (layers): ModuleList(\n",
              "            (0-27): 28 x Qwen2VLDecoderLayer(\n",
              "              (self_attn): Qwen2VLAttention(\n",
              "                (q_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (k_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (v_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (o_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (rotary_emb): Qwen2VLRotaryEmbedding()\n",
              "              )\n",
              "              (mlp): Qwen2MLP(\n",
              "                (gate_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=18944, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (up_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=18944, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (down_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=18944, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act_fn): SiLU()\n",
              "              )\n",
              "              (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "              (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "            )\n",
              "          )\n",
              "          (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "          (rotary_emb): Qwen2VLRotaryEmbedding()\n",
              "        )\n",
              "      )\n",
              "      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "FastVisionModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmifFWEPqP1G",
        "outputId": "7fd60521-11d4-4830-a0a4-f0e740fc85b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Model does not have a default image size - using 512\n"
          ]
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer=tokenizer,\n",
        "    #function of data collator is to ensure that the batches are of uniform length for image and text\n",
        "    data_collator = UnslothVisionDataCollator(model, tokenizer),\n",
        "    train_dataset = converted_dataset,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps = 10,\n",
        "        max_steps=50,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bf16_supported(),\n",
        "        bf16 = is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to = \"none\",\n",
        "        remove_unused_columns=False,\n",
        "        dataset_text_field=\"\",\n",
        "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
        "        dataset_num_proc=4,\n",
        "        max_seq_length=2048,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BNj0lG1XqP8L",
        "outputId": "55a71914-526f-4b57-fd3c-63ac757bc8ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 68,686 | Num Epochs = 1 | Total steps = 50\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 50,855,936 of 8,342,231,552 (0.61% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 04:34, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.184700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.332600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.440700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.052700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.047100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.139900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.992600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.714800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.504700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.536600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.449100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.352000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.315300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.198300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.200500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.121500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.094800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.103400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.089500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.198800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.171100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.172600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.147800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.134900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.179000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.144100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.088300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.297300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.079400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.151800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.199400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.091200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.138100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.139500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.044600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.125400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.123000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.109700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.102500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.131500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.110600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.126100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.100700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.211900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.195400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.090800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.153800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.134500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.097700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.168800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=50, training_loss=0.3246185152232647, metrics={'train_runtime': 335.0653, 'train_samples_per_second': 1.194, 'train_steps_per_second': 0.149, 'total_flos': 2721858506158080.0, 'train_loss': 0.3246185152232647, 'epoch': 0.005823603063215211})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jTNAE-wjqQEX",
        "outputId": "c91609ec-ada2-4132-d864-e83d8a5825aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Qwen2VLForConditionalGeneration(\n",
              "      (model): Qwen2VLModel(\n",
              "        (visual): Qwen2VisionTransformerPretrainedModel(\n",
              "          (patch_embed): PatchEmbed(\n",
              "            (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
              "          )\n",
              "          (rotary_pos_emb): VisionRotaryEmbedding()\n",
              "          (blocks): ModuleList(\n",
              "            (0-18): 19 x Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (19): Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (20-21): 2 x Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (22): Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (23-28): 6 x Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (29): Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (30-31): 2 x Qwen2VLVisionBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "              (attn): VisionAttention(\n",
              "                (qkv): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (proj): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (mlp): VisionMlp(\n",
              "                (fc1): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act): QuickGELUActivation()\n",
              "                (fc2): lora.Linear(\n",
              "                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (merger): PatchMerger(\n",
              "            (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "              (1): GELU(approximate='none')\n",
              "              (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (language_model): Qwen2VLTextModel(\n",
              "          (embed_tokens): Embedding(152064, 3584, padding_idx=151654)\n",
              "          (layers): ModuleList(\n",
              "            (0-27): 28 x Qwen2VLDecoderLayer(\n",
              "              (self_attn): Qwen2VLAttention(\n",
              "                (q_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (k_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (v_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (o_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (rotary_emb): Qwen2VLRotaryEmbedding()\n",
              "              )\n",
              "              (mlp): Qwen2MLP(\n",
              "                (gate_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=18944, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (up_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=18944, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (down_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Identity()\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=18944, out_features=16, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act_fn): SiLU()\n",
              "              )\n",
              "              (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "              (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "            )\n",
              "          )\n",
              "          (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "          (rotary_emb): Qwen2VLRotaryEmbedding()\n",
              "        )\n",
              "      )\n",
              "      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "FastVisionModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "iXh4m3dqqQH5"
      },
      "outputs": [],
      "source": [
        "image = dataset[3][\"image\"]\n",
        "instruction = \"Write the LaTeX representation for this image.\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": [\n",
        "        {\"type\": \"image\"},\n",
        "        {\"type\": \"text\", \"text\": instruction}\n",
        "    ]}\n",
        "]\n",
        "\n",
        "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = tokenizer(\n",
        "    image,\n",
        "    input_text,\n",
        "    add_special_tokens=False,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8y9WKTMqQNM",
        "outputId": "5964d7e4-bc10-41b4-e01c-8ee2926c6264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\sigma ^ { \\mu } \\frac { \\lambda ^ { a } } { 2 } A ^ { \\mu } ^ { a } .<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True, temperature=1.5, min_p=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g4tjSrRC1Fhi",
        "outputId": "b7616123-dc7b-41f1-b6bc-6c8efa2b82c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (159 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1 rapidfuzz-3.14.1\n"
          ]
        }
      ],
      "source": [
        "# Install the required library for edit distance calculation\n",
        "!pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OtBo-tO1FuX",
        "outputId": "080d8083-83b1-4ccd-e24a-6df30932f219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Model on 100 Samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [15:28<00:00,  9.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "       FINAL EVALUATION METRICS (100 Samples)\n",
            "==================================================\n",
            "Total Samples Evaluated: 100\n",
            "Average Normalized Edit Distance (NED): 0.2067\n",
            "Average Character Accuracy (1 - NED): 79.33%\n",
            "==================================================\n",
            "Interpretation: NED is the character-level error rate. Lower is better.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "# Assume FastVisionModel, tokenizer, and model are already defined and loaded\n",
        "\n",
        "# --- 3. Load the Evaluation Dataset (First 100 Samples of Test Split) ---\n",
        "eval_dataset = load_dataset(\"unsloth/Latex_OCR\", split=\"test[:100]\") # Evaluates on 100 samples\n",
        "\n",
        "# --- 4. Define the Evaluation Metric Function (NED) ---\n",
        "def calculate_ned(predicted: str, reference: str) -> float:\n",
        "    \"\"\"Calculates Normalized Edit Distance (NED) based on Levenshtein distance.\"\"\"\n",
        "    # Clean up whitespace and special characters\n",
        "    predicted = predicted.strip()\n",
        "    reference = reference.strip()\n",
        "\n",
        "    lev_distance = levenshtein_distance(predicted, reference)\n",
        "    max_len = max(len(predicted), len(reference))\n",
        "\n",
        "    # Handle empty strings case\n",
        "    if max_len == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return lev_distance / max_len\n",
        "\n",
        "# --- 5. Run Inference on the Evaluation Set ---\n",
        "FastVisionModel.for_inference(model) # Ensure model is in inference mode\n",
        "model.eval()\n",
        "\n",
        "all_neds = []\n",
        "instruction = \"Write the LaTeX representation for this image.\"\n",
        "\n",
        "# Loop through the 100 samples in the evaluation dataset\n",
        "for sample in tqdm(eval_dataset, desc=\"Evaluating Model on 100 Samples\"):\n",
        "    image = sample[\"image\"]\n",
        "    reference_latex = sample[\"text\"]\n",
        "\n",
        "    # Format the multi-modal chat template\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": instruction}\n",
        "        ]}\n",
        "    ]\n",
        "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "    # Tokenize the input and move to GPU\n",
        "    inputs = tokenizer(\n",
        "        image,\n",
        "        input_text,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Generate the output (using low temperature for deterministic evaluation)\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            use_cache=True,\n",
        "            temperature=0.1,\n",
        "            min_p=0.1,\n",
        "            do_sample=True,\n",
        "        )\n",
        "\n",
        "    # Decode and extract the predicted LaTeX\n",
        "    predicted_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Robustly extract the assistant's final response (the LaTeX code)\n",
        "    try:\n",
        "        assistant_prefix = 'assistant\\n'\n",
        "        predicted_latex = predicted_text.split(assistant_prefix)[-1].strip()\n",
        "        if tokenizer.eos_token in predicted_latex:\n",
        "            predicted_latex = predicted_latex.split(tokenizer.eos_token)[0].strip()\n",
        "    except Exception:\n",
        "        predicted_latex = predicted_text.strip()\n",
        "\n",
        "    # Calculate NED for the sample\n",
        "    ned_score = calculate_ned(predicted_latex, reference_latex)\n",
        "    all_neds.append(ned_score)\n",
        "\n",
        "# --- 6. Final Results Calculation and Output ---\n",
        "if all_neds:\n",
        "    average_ned = sum(all_neds) / len(all_neds)\n",
        "    average_accuracy = 1.0 - average_ned\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"       FINAL EVALUATION METRICS (100 Samples)\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total Samples Evaluated: {len(eval_dataset)}\")\n",
        "    print(f\"Average Normalized Edit Distance (NED): {average_ned:.4f}\")\n",
        "    print(f\"Average Character Accuracy (1 - NED): {average_accuracy * 100:.2f}%\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Interpretation: NED is the character-level error rate. Lower is better.\")\n",
        "else:\n",
        "    print(\"\\nNo samples were evaluated. Please check the dataset loading.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}